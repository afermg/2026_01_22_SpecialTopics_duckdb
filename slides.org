#+startup: beamer
#+TITLE: Special topics: DuckDB at scale
#+date: 2026/01/22
#+author: Alán F. Muñoz
#+OPTIONS: ^:nil num:t  H:2 \n:t
#+LaTeX_CLASS: beamer
#+BEAMER_THEME: moloch
# #+LaTeX_CLASS_OPTIONS: [t]
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)
# #+LATEX_HEADER: \institute{Carpenter-Singh Lab, Broad Institute of MIT and Harvard}
# #+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{pmboxdraw}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \newunicodechar{└}{\textSFii}
#+LATEX_HEADER: \newunicodechar{├}{\textSFviii}
#+LATEX_HEADER: \newunicodechar{─}{\textSFx}
#+LATEX_HEADER: \newunicodechar{┐}{\textSFiii}
#+LATEX_HEADER: \newunicodechar{└}{\textSFii}
#+LATEX_HEADER: \newunicodechar{┴}{\textSFvii}
#+LATEX_HEADER: \newunicodechar{┬}{\textSFvi}
#+LATEX_HEADER: \newunicodechar{├}{\textSFviii}
#+LATEX_HEADER: \newunicodechar{─}{\textSFx}
#+LATEX_HEADER: \newunicodechar{│}{\textSFxi}
#+LATEX_HEADER: \newunicodechar{┼}{\textSFv}
#+LATEX_HEADER: \newunicodechar{┌}{\textSFi}
#+LATEX_HEADER: \newunicodechar{┘}{\textSFiv}
#+LATEX_HEADER: \newunicodechar{┤}{\textSFix}
#+LATEX_HEADER: \newunicodechar{…}{{\ldots}}
#+LATEX_HEADER: \newcommand\monoellipsis{\hbox to.5em{\hss.\hss\hss.\hss\hss.\hss}}
#+LATEX_HEADER: \newunicodechar{…}{{\monoellipsis}} % One-character ellipsis unicode U+2026
#+LATEX_HEADER: \newunicodechar{ }{\ } % Non-breaking space, unicode c2a0
#+LATEX_HEADER: \newunicodechar{·}{\textperiodcentered} % Center dot unicode U+00B7
# #+LATEX_HEADER: \usepackage[scaled=1.0]{DejaVuSansMono}
#+LATEX_HEADER: \usepackage[scaled=1.0]{inconsolata}
#+LATEX_HEADER: \setbeamercovered{transparent=0.1}
#+latex_header: \setminted{style=solarized-light,frame=leftline}
#+LATEX_HEADER: \usepackage{animate}
#+PROPERTY: :eval no-export

* Introduction
** What is Duckdb
DuckDB is an open source in-process SQL engine optimised for analytics queries.

- 'In-process' means it runs within your application. You don't need to start a separate service (e.g., Postgres).
- 'Optimised for analytics queries' means that it's designed joins and aggregations involving large numbers of rows.

** Key features
- Speed ("Ridiculously fast")
- No dependencies!
- SQL (+ improvements)
- Supports all important filetypes (e.g., csv, parquet, sql)
- User-Defined Functions (UDF) + community extensions
- LLM-friendly documentation

** In which cases is it useful?
- Local data exploration
- Large datasets
- Query remote data without copying

It can be 100-1000 times faster than SQLite/Postgres.

** The quintessential use-case
1. Read =csv=, =parquet=, =json=.
2. Clean, join, aggregate, math operations, create new columns

** Common processing tools in data science
#+ATTR_LATEX: :width 0.95 \linewidth
[[./figs/comparison_table.png]]

* Case study: The MBTA open data
** The Open Data portal for the Massachusets Bay Transport Authority 
#+ATTR_LATEX: :width 0.95 \linewidth
[[./figs/mbta_opendata_screenshot.png]]
** First we adjust the outputs for slide-like sizes
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
#+begin_src sql :session duckdb :db test.duckdb :exports code :results none :eval no-export
  -- Remote calls
  INSTALL httpfs; 
  LOAD httpfs;
  
  -- Slides-friendly visualisation
  .maxrows 10
  .maxwidth 20
  .mode box

  SET VARIABLE line_data = 'https://hub.arcgis.com/api/v3/datasets/
  a2d15ddd86b34867a31cd4b8e0a83932_0/
  downloads/data?format=csv&spatialRefId=4326&where=1%3D1'
#+end_src

** Overview of one of the tables
:PROPERTIES:
:header-args:duckdb: :session duckdb :db test.duckdb :exports both
:EXPORT_FILE_NAME: duckdb-exploration-mbta
:END:

There are multiple available (tabular) datasets:
- Ridership by Trip, Route line and stop
- Montly ridership by month
- Gated station entries
- Passenger surveys

** Let's explore the ridership table
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
CREATE OR REPLACE TABLE monthly_ridership AS (
FROM read_csv(getvariable('line_data')));
SELECT #1,#2 FROM (DESCRIBE monthly_ridership);
#+END_SRC

#+RESULTS:
#+begin_example
┌─────────────────────────────────┬──────────────────────────┐
│           column_name           │       column_type        │
├─────────────────────────────────┼──────────────────────────┤
│ service_date                    │ TIMESTAMP WITH TIME ZONE │
│ mode                            │ VARCHAR                  │
│ route_or_line                   │ VARCHAR                  │
│ total_monthly_weekday_ridership │ BIGINT                   │
│ average_monthly_weekday_ridersh │ BIGINT                   │
│ countofdates_weekday            │ BIGINT                   │
│ total_monthly_ridership         │ DOUBLE                   │
│ average_monthly_ridership       │ BIGINT                   │
│ countofdates                    │ BIGINT                   │
│ ObjectId                        │ BIGINT                   │
└─────────────────────────────────┴──────────────────────────┘
#+end_example

** List the unique route or lines
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
  FROM monthly_ridership
  SELECT DISTINCT route_or_line
  ORDER BY route_or_line
#+END_SRC

#+RESULTS:
#+begin_example
┌───────────────┐
│ route_or_line │
│    varchar    │
├───────────────┤
│ Blue Line     │
│ Boat-F1       │
│ Boat-F3       │
│ Boat-F4       │
│ Bus           │
│ Commuter Rail │
│ Green Line    │
│ Orange Line   │
│ Red Line      │
│ Silver Line   │
│ The RIDE      │
├───────────────┤
│    11 rows    │
└───────────────┘
#+end_example

** What is the ridership by route or line?
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
We will thus group by route or line to see which routes move around the 
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
SELECT route_or_line, CAST(MEAN(total_monthly_weekday_ridership) AS INTEGER)
AS mean_weekly_ridership FROM monthly_ridership GROUP BY route_or_line
ORDER BY mean_weekly_ridership DESC;
#+END_SRC

#+RESULTS:
#+begin_example
┌───────────────┬───────────────────────┐
│ route_or_line │ mean_weekly_ridership │
│    varchar    │         int32         │
├───────────────┼───────────────────────┤
│ Bus           │               7395469 │
│ Red Line      │               5326706 │
│ Orange Line   │               4410355 │
│ Green Line    │               3893135 │
│ Commuter Rail │               2684943 │
│ Blue Line     │               1392658 │
│ Silver Line   │                724622 │
│ The RIDE      │                138221 │
│ Boat-F1       │                 66248 │
│ Boat-F3       │                 23922 │
│ Boat-F4       │                 19975 │
├───────────────┴───────────────────────┤
│ 11 rows                     2 columns │
└───────────────────────────────────────┘
#+end_example

** Let us look at a table with per-station entries
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
This other dataset contains per-station information
#+begin_src sql :session duckdb :db test.duckdb :exports code :eval no-export :results none
SET VARIABLE gates_data = 'https://hub.arcgis.com
/api/v3/datasets /001c177f07594e7c99f193dde32284c9_0
/downloads/data?format=csv&spatialRefId=4326&where=1%3D1';
#+end_src
** Information of specific entrances
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:

#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
CREATE OR REPLACE TABLE gated_entries AS (
FROM read_csv(getvariable('gates_data')));
SELECT #1,#2 FROM (DESCRIBE gated_entries);
#+END_SRC

#+RESULTS:
#+begin_example
┌───────────────┬──────────────────────────┐
│  column_name  │       column_type        │
│    varchar    │         varchar          │
├───────────────┼──────────────────────────┤
│ service_date  │ TIMESTAMP WITH TIME ZONE │
│ time_period   │ VARCHAR                  │
│ stop_id       │ VARCHAR                  │
│ station_name  │ VARCHAR                  │
│ route_or_line │ VARCHAR                  │
│ gated_entries │ DOUBLE                   │
│ ObjectId      │ BIGINT                   │
└───────────────┴──────────────────────────┘
#+end_example

** Which station has the most recorded gate entries?
:PROPERTIES:
:BEAMER_opt: shrink=12
:END:
\pause
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
SELECT station_name,route_or_line,CAST(SUM(gated_entries) AS INT)
AS gated_entries FROM gated_entries
GROUP BY station_name, route_or_line
ORDER BY gated_entries DESC
#+END_SRC

#+RESULTS:
#+begin_example
┌─────────────────┬───────────────┬───────────────┐
│  station_name   │ route_or_line │ gated_entries │
│     varchar     │    varchar    │     int32     │
├─────────────────┼───────────────┼───────────────┤
│ Harvard         │ Red Line      │       4802562 │
│ Back Bay        │ Orange Line   │       4253244 │
│ Copley          │ Green Line    │       3768772 │
│ North Station   │ Orange Line   │       3686858 │
│ Central         │ Red Line      │       3640045 │
│    ·            │    ·          │           ·   │
│    ·            │    ·          │           ·   │
│    ·            │    ·          │           ·   │
│ Suffolk Downs   │ Blue Line     │        234871 │
│ Union Square    │ Green Line    │        181843 │
│ Ball Square     │ Green Line    │        169984 │
│ Magoun Square   │ Green Line    │        148869 │
│ East Somerville │ Green Line    │         89718 │
├─────────────────┴───────────────┴───────────────┤
│ 78 rows (10 shown)                    3 columns │
└─────────────────────────────────────────────────┘
#+end_example

** The one-two: Group and calculate
Most data processing can be reduced to:
1. Group relevant categories
- Treatment vs controls
- Treatments vs each other
2. Math/Logical operation on groups
- Basic operations (e.g., $+-*/$)
- Vector operations (e.g., cosine similarity)
- Logical operations (e.g., if-else)
   - A combination of the above
3. Save into a new dataset for further processing/viz
   
* Tools for large-scale data
** What do we consider large-scale?
- Data larger than an off-the-shelf server
- Processing takes time requires memory/storage considerations
- Traditionally we would use cloud compute
** Problem: Many different groups and subgroups
Most of our data is highly structured:
- By Metadata: Source -> Batch -> Plate -> Well -> Site
- By annotation: Perturbation id, Control vs perturbation
  
** Abstraction: Tables vs Trees
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:

Relational databases are a battle-proven technology, but are not always optimal for heavily structured data.
#+begin_src sql :session duckdb :db test.duckdb :exports both :results raw :wrap :eval no-export 
  SET VARIABLE plates = 'https://github.com/jump-cellpainting/datasets/
  raw/refs/heads/duckdb/metadata/plate.csv.gz';
  FROM getvariable(plates) LIMIT 3;
#+end_src

#+RESULTS:
#+begin_example
┌─────────────────┬─────────────────┬────────────────┬────────────────────┐
│ Metadata_Source │ Metadata_Batch  │ Metadata_Plate │ Metadata_PlateType │
│     varchar     │     varchar     │    varchar     │      varchar       │
├─────────────────┼─────────────────┼────────────────┼────────────────────┤
│ source_1        │ Batch1_20221004 │ UL000109       │ COMPOUND_EMPTY     │
│ source_1        │ Batch1_20221004 │ UL001641       │ COMPOUND           │
│ source_1        │ Batch1_20221004 │ UL001643       │ COMPOUND           │
└─────────────────┴─────────────────┴────────────────┴────────────────────┘
#+end_example

** We can express it as a nested tree
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:

The simplest way to transform table to nested I could come up with.
#+begin_src sql :session duckdb :eval no-export :exports code
  COPY (SELECT json_object(
      Metadata_Source, json_group_array(sub_data)
  ) AS root
  FROM (
      SELECT Metadata_Source, Metadata_Batch,
      json_object(Metadata_Batch,
      json_group_array(json_object(Metadata_Plate, Metadata_PlateType))
      ) AS sub_data
      FROM getvariables(plates)
      GROUP BY Metadata_Source, Metadata_Batch, Metadata_Plate
  )
  GROUP BY Metadata_Source) TO 'output.json';
#+end_src

** The nested tree makes one type of (key-based) access more efficient
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
At the cost basically all others.

#+begin_src sh :results verbatim :wrap example :eval no-export :exports both
  jq '.root | walk(if type == "array" then add else . end) |
  with_entries(select(.key == "source_1")) | select(. != {})'
  output.json | head -n 11
#+end_src

#+RESULTS:
#+begin_example
{
  "source_1": {
    "Batch1_20221004": {
      "UL000109": "COMPOUND_EMPTY"
    },
    "Batch2_20221006": {
      "UL001675": "COMPOUND"
    },
    "Batch6_20221102": {
      "UL000599": "COMPOUND"
    },
#+end_example

** Using multiple tables and schemas can help 
 But it is untracktable when the database size exceeds 10s of GBs and must be loaded into memory all at once for queries.
** Problem: Massive hierarchical datasets
There is no clear path: One big database vs a myriad of files.

** Potential solution: Parquet hive partitioning
These prioritise keys that optimise the most common queries.

#+ATTR_LATEX: :width 0.45 \linewidth
[[./figs/hive_partitioning.png]]

** Hive partitioning structure
:PROPERTIES:
:BEAMER_opt: shrink=10
:END:
Single-object (e.g., cell, nuclei) time series data.

#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  DATA_DIR="/datastore/alan/gsk/aliby_output/ELN201687/
  H00DJKJread1BF48hrs_20230926_095825hive_profiles_tracked"

   tree $DATA_DIR | sed -n "2,10p" 
#+end_src

#+RESULTS:
#+begin_example
├── site=A01_001
│   ├── object=cell
│   │   └── data_0.parquet
│   └── object=nuclei
│       └── data_0.parquet
├── site=A01_002
│   ├── object=cell
│   │   └── data_0.parquet
│   └── object=nuclei
#+end_example

** The profiles are big enough to become problematic
The experiment contains 384 wells \cdot 5 sites \cdot 2 objects \cdot 20 time points \approx 77k files with single-cell profiles.
#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  du -sh ${DATA_DIR} | cut -f 1
#+end_src

#+RESULTS:
#+begin_example
20G
#+end_example

** We treat key columns like any other column
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
The full table shows a "tidy"-like structure for morphological features. In reality 
#+begin_src sql :eval no-export :session duckdb :wrap example :results verbatim  
  SET VARIABLE hive = '/datastore/alan/gsk/aliby_output/ ELN201687/
  H00DJKJread1BF48hrs_20230926_095825hive_profiles_tracked/
  */*/*.parquet';

  SELECT #1,#2 FROM (DESCRIBE FROM
  read_parquet(getvariable('hive'), hive_partitioning = true));
#+end_src

#+RESULTS:
#+begin_example
┌─────────────┬─────────────┐
│ column_name │ column_type │
│   varchar   │   varchar   │
├─────────────┼─────────────┤
│ tile        │ BIGINT      │
│ label       │ BIGINT      │
│ branch      │ VARCHAR     │
│ metric      │ VARCHAR     │
│ value       │ DOUBLE      │
│ tp          │ UTINYINT    │
│ filename    │ VARCHAR     │
│ fname       │ VARCHAR     │
│ site_1      │ VARCHAR     │
│ object      │ VARCHAR     │
│ site        │ VARCHAR     │
├─────────────┴─────────────┤
│ 11 rows         2 columns │
└───────────────────────────┘
#+end_example

** Hive partitioning
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:
Duckdb will only load the file(s) that are relevant to our query, saving time and memory.
#+begin_src sql :eval no-export :session duckdb :wrap example :results verbatim :exports both 
  SET VARIABLE hive = '/datastore/alan/gsk/aliby_output/ELN201687/
  H00DJKJread1BF48hrs_20230926_095825hive_profiles_tracked/*/*/*.parquet';
  SELECT branch,site,metric,value FROM (
  FROM read_parquet(getvariable('hive'), hive_partitioning = true)
  WHERE site LIKE 'B0%' AND object='cell' ORDER BY random())
  USING SAMPLE 3;
#+end_src

#+RESULTS:
#+begin_example
┌──────────────────────┬─────────┬───────────────────────┬─────────────────────┐
│        branch        │  site   │        metric         │        value        │
│       varchar        │ varchar │        varchar        │       double        │
├──────────────────────┼─────────┼───────────────────────┼─────────────────────┤
│ 1/max/texture        │ B04_001 │ SumAverage_3_02_256   │  119.70689655172414 │
│ 1/max/radial_zerni…  │ B01_002 │ RadialDistribution_…  │ 0.43083862789046273 │
│ 0/max/radial_zerni…  │ B04_003 │ RadialDistribution_…  │  0.6837160487262536 │
└──────────────────────┴─────────┴───────────────────────┴─────────────────────┘
#+end_example

** Memory profiling experiments: Data selection and shuffling
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:
Site B01_001 (1 file, directly read): 56MB of peak usage. 

#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  command time --format='%e seconds; %M max memory (KB)' duckdb -c "
  SELECT branch,site,metric,value FROM (
  FROM read_parquet('${DATA_DIR}/site=B01_001/object=cell/data_0.parquet')
  WHERE site = 'B01_001' AND object = 'cell'
  ORDER BY random()) USING SAMPLE 3;"
#+end_src

#+RESULTS:
#+begin_example
┌─────────────────┬─────────┬─────────────┬───────────────────────┐
│     branch      │  site   │   metric    │         value         │
│     varchar     │ varchar │   varchar   │        double         │
├─────────────────┼─────────┼─────────────┼───────────────────────┤
│ 1/max/zernike   │ B01_001 │ Zernike_6_4 │ 0.0038834561835829843 │
│ 0/max/sizeshape │ B01_001 │ Center_X    │     693.9240674955595 │
│ 1/max/sizeshape │ B01_001 │ FormFactor  │    0.7671706171485779 │
└─────────────────┴─────────┴─────────────┴───────────────────────┘
0.05 seconds; 56956 max memory (KB)
#+end_example

** Memory profiling experiments: Data selection and shuffling
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:

Site B01_001 (1 file): 720MB peak usage

#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  command time --format='%e seconds; %M max memory (KB)' duckdb -c "
  SELECT branch,site,metric,value FROM (
  FROM read_parquet('${HIVE}', hive_partitioning = true)
  WHERE site = 'B01_001' AND object='cell'
  ORDER BY random()) USING SAMPLE 3;"
#+end_src

#+RESULTS:
#+begin_example
┌─────────────────┬─────────┬────────────────────┬───────────────────────┐
│     branch      │  site   │       metric       │         value         │
│     varchar     │ varchar │      varchar       │        double         │
├─────────────────┼─────────┼────────────────────┼───────────────────────┤
│ 1/max/zernike   │ B01_001 │ Zernike_5_3        │  0.018311454640108134 │
│ 1/max/sizeshape │ B01_001 │ CentralMoment_2_2  │    287848640.24825424 │
│ 0/max/texture   │ B01_001 │ InfoMeas1_3_00_256 │ -0.008348726106163183 │
└─────────────────┴─────────┴────────────────────┴───────────────────────┘
0.44 seconds; 723624 max memory (KB)
#+end_example

** Memory profiling experiments: Data selection and shuffling
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:
Site B01_00% (5 files): ~1.7 GB peak memory usage.

#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  command time --format='%e seconds; %M max memory (KB)' duckdb -c "
  SELECT branch,site,metric,value FROM (
  FROM read_parquet('${HIVE}', hive_partitioning = true)
  WHERE site LIKE 'B01_00%' AND object='cell'
  ORDER BY random()) USING SAMPLE 3;"
#+end_src

#+RESULTS:
#+begin_example
┌─────────────────┬─────────┬──────────────────────┬────────────────────┐
│     branch      │  site   │        metric        │       value        │
│     varchar     │ varchar │       varchar        │       double       │
├─────────────────┼─────────┼──────────────────────┼────────────────────┤
│ 1/max/sizeshape │ B01_001 │ SpatialMoment_2_0    │          4084235.0 │
│ 0/max/sizeshape │ B01_003 │ NormalizedMoment_2_3 │ 0.2893527115553125 │
│ 0/max/sizeshape │ B01_005 │ InertiaTensor_1_1    │          38.509375 │
└─────────────────┴─────────┴──────────────────────┴────────────────────┘
0.55 seconds; 1711220 max memory (KB)
#+end_example

** Memory profiling experiments: Data selection and shuffling
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:

Site B0% (45 files): ~14GB peak memory
#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  command time --format='%e seconds; %M max memory (KB)' duckdb -c "
  SELECT branch,site,metric,value FROM (
  FROM read_parquet('${HIVE}', hive_partitioning = true)
  WHERE site LIKE 'B0%' AND object='cell'
  ORDER BY random()) USING SAMPLE 3;"
#+end_src

#+RESULTS:
#+begin_example
┌─────────────────┬─────────┬──────────────────────┬──────────────────────┐
│     branch      │  site   │        metric        │        value         │
│     varchar     │ varchar │       varchar        │        double        │
├─────────────────┼─────────┼──────────────────────┼──────────────────────┤
│ 0/max/zernike   │ B01_001 │ Zernike_7_5          │ 0.002459246938190699 │
│ 1/max/sizeshape │ B07_003 │ SpatialMoment_2_0    │           23596393.0 │
│ 0/max/sizeshape │ B01_001 │ BoundingBoxMaximum_Y │                642.0 │
└─────────────────┴─────────┴──────────────────────┴──────────────────────┘
1.70 seconds; 14132836 max memory (KB)
#+end_example

** Disadvantages of Hive partitioning
*** Advantages
- Faster, enables concurrent write
  Partitions can be written without
- Lower memory footprint
  Only relevant partitions are read into memory
- Faster queries
  Optimisation occurs at the IO level
*** Disadvantages
- Many small files may impact the performance of very small and very large queries.
- Compression penalty: Compression occurs at the most granular level.
  
** Problem: But my vectors! :(
SQL was invented before vector operations became the pillar of civilization.
** Solution: Vector Similarity extension
#+ATTR_LATEX: :width 0.95 \linewidth
[[./figs/vector_sim_duckdb_screenshot.png]]
** Caveat: GPU processing will usually be faster for round-robin comparisons
[[./figs/pandas_cudf_benchmark.png]]
** Problem: I need a function to process data, but it's not available in duckdb
Normally this would entail:
1. Loading data into PythonLand and doing slow processing
2. Moving it back to duckdb and resuming data wrangling
   
** Solution 1: User-Defined functions
[[https://duckdb.org/docs/stable/clients/python/function][source]]
[[./figs/udf_duckdb.png]]
** Solution 2: Duckdb-friendly multithreading
[[https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/multithreading-and-parallelism/multithreading-and-parallelism-python/#connections-in-multiple-threads][source]]
[[./figs/multithreading_duckdb.png]]
** Some previous use-cases at the CS-Lab
- Dealing with tens/hundreds of GB of data processing for GSK project
- =copairs= 10-100x speedup
- On-browser cosine similarity between chemical embeddings
- /Claudetanu/ uses it extensively to structure JUMP metadata
  
* Conclusions
** Duckdb solves multiple problems
- In-memory OR in disk
- Larger-than memory data
- Multithreading/Multiprocessing
- Optimised queries
- Web requests
- Vector operations
  
** Rules of the thumb for DuckDB vs GPU
Prefer matrix operations (ideally over GPUs) when
- You need pairwise mathematical operations AND
- You are already in a Python GPU-enabled environment

Otherwise, duckdb is likely the cost and time-efficient approach.

** The ecosystem is still growing
- DuckLake (format): Single repository of data stored in its natural format
- MotherDuck (startup): Duckdb-based data warehouse in SQL or natural language
  
** I'm excited (or not)! How can I try it out?
From within Python: PyPI's =duckdb= package.
*** In my computers/servers:
- Already on CS-Lab Nix servers
- MacOS: =brew= or Download from =duckdb.org=
- Windows: =duckdb.org=
*** I want a UI!
- The CLI comes with one: =duckdb --ui=
- =Marimo= supports =duckdb= out of the box!
- Jupyter notebooks running =duckdb=
  
** Thanks for your attention
\center
\animategraphics[loop,controls,width=0.6\linewidth]{8}{figs/waddle/waddle_}{1}{6}

** Resources/Blogs
- [[https://www.robinlinacre.com/recommend_duckdb/][Why Duckdb is my first choice for data processing]]
- [[https://www.dbreunig.com/2025/05/03/duckdb-is-the-most-impactful-geospatial-software-in-a-decade.html][DuckDB is Probably the Most Important Geospatial Software of the Last Decade]]
- [[https://hakibenita.com/sql-for-data-analysis][Practical SQL for Data Analysis]]
- [[https://duckdb.org/2024/05/03/vector-similarity-search-vss][Vector Similarity Search in DuckDB]]
- [[https://spectrum.ieee.org/the-rise-of-sql][The Rise of SQL]]
- [[https://www.scattered-thoughts.net/writing/against-sql/][Against SQL]]
- [[https://duckdb.org/2024/05/03/vector-similarity-search-vss][Vector Similarity Search using DuckDB]]
  Benchmarks
- [[https://docs.rapids.ai/api/cudf/stable/cudf_pandas/benchmarks/][cudf-pandas]]
- [[https://blog.lordpatil.com/posts/pandas-polars-duckdb-performance-comparison/][pandas vs polars vs duckdb]]
