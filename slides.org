#+startup: beamer
#+TITLE: Special topics: DuckDB at scale
#+date: 2026/01/22
#+author: Alán F. Muñoz
#+OPTIONS: ^:nil num:t  H:2 \n:t
#+LaTeX_CLASS: beamer
#+BEAMER_THEME: moloch
# #+LaTeX_CLASS_OPTIONS: [t]
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)
# #+LATEX_HEADER: \institute{Carpenter-Singh Lab, Broad Institute of MIT and Harvard}
# #+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
% 
#+LATEX_HEADER: \usepackage{pmboxdraw}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \newunicodechar{└}{\textSFii}
#+LATEX_HEADER: \newunicodechar{├}{\textSFviii}
#+LATEX_HEADER: \newunicodechar{─}{\textSFx}
#+LATEX_HEADER: \newunicodechar{┐}{\textSFiii}
#+LATEX_HEADER: \newunicodechar{└}{\textSFii}
#+LATEX_HEADER: \newunicodechar{┴}{\textSFvii}
#+LATEX_HEADER: \newunicodechar{┬}{\textSFvi}
#+LATEX_HEADER: \newunicodechar{├}{\textSFviii}
#+LATEX_HEADER: \newunicodechar{─}{\textSFx}
#+LATEX_HEADER: \newunicodechar{│}{\textSFxi}
#+LATEX_HEADER: \newunicodechar{┼}{\textSFv}
#+LATEX_HEADER: \newunicodechar{┌}{\textSFi}
#+LATEX_HEADER: \newunicodechar{┘}{\textSFiv}
#+LATEX_HEADER: \newunicodechar{┤}{\textSFix}
#+LATEX_HEADER: \newunicodechar{…}{{\ldots}}
#+LATEX_HEADER: \newcommand\monoellipsis{\hbox to.5em{\hss.\hss\hss.\hss\hss.\hss}}
#+LATEX_HEADER: \newunicodechar{…}{{\monoellipsis}} % One-character ellipsis unicode U+2026
#+LATEX_HEADER: \newunicodechar{ }{\ } % Non-breaking space, unicode c2a0
#+LATEX_HEADER: \newunicodechar{·}{\textperiodcentered} % Center dot unicode U+00B7
# #+LATEX_HEADER: \usepackage[scaled=1.0]{DejaVuSansMono}
#+LATEX_HEADER: \usepackage[scaled=1.0]{inconsolata}
#+LATEX_HEADER: \setbeamercovered{transparent=100}
#+latex_header: \setminted{style=solarized-light,frame=leftline}
#+PROPERTY: :eval no-export

* Introduction
** What is Duckdb
DuckDB is an open source in-process SQL engine optimised for analytics queries.

- 'In-process' means it runs within your application. You don't need to start a separate service (e.g., Postgres).
- 'Optimised for analytics queries' means that it's designed joins and aggregations involving large numbers of rows.

** In which cases is it useful?
- Local data exploration
- Large datasets
- Query remote data without copying

It can be 100-1000 times faster than SQLite/Postgres.

** A core use-case
1. Read =csv=, =parquet=, =json=.
2. Clean, join, aggregate, math operations, create new columns

** Key features
- Speed ("Ridiculously fast")
- No dependencies!
- SQL (+ improvements)
- Supports all important filetypes (e.g., csv, parquet, sql)
- User-Defined Functions (UDF) + community extensions
- LLM-friendly documentation

** Common tools in data science
| Software | Note                     | Language    |
|----------+--------------------------+-------------|
| Pandas   | Popular                  | Py/Cython/C |
| Polars   | Lazy & Fast              | Rust        |
| SQLite   | Ubiquituous              | C           |
| Duckdb   | Lazy, Fast and versatile | C++         |

** Why a database language if what we use is DataFrames?
DataFrames were a mistake.

** Hot take: Learning polars/pandas/ibis API is not worth it
All support sql-like.


* Case study: The MBTA open data
** The Open Data portal for the Massachusets Bay Transport Authority 
#+ATTR_LATEX: :width 0.95 \linewidth
[[./figs/mbta_opendata_screenshot.png]]
** First we adjust the outputs for slide-like sizes
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
#+begin_src sql :session duckdb :db test.duckdb :exports code :results none :eval no-export
  -- Remote calls
  INSTALL httpfs; 
  LOAD httpfs;

  -- Slides-friendly visualisation
  .maxrows 10
  .maxwidth 20
  .mode box

  SET VARIABLE line_data = 'https://hub.arcgis.com/api/v3/datasets/a2d15ddd86b34867a31cd4b8e0a83932_0/downloads/data?format=csv&spatialRefId=4326&where=1%3D1'
#+end_src

** Overview of one of the tables
:PROPERTIES:
:header-args:duckdb: :session duckdb :db test.duckdb :exports both
:EXPORT_FILE_NAME: duckdb-exploration-mbta
:END:

There are multiple available (tabular) datasets:
- Ridership by Trip, Route line and stop
- Montly ridership by month
- Gated station entries
- Passenger surveys

** Let's explore the ridership table
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
CREATE OR REPLACE TABLE monthly_ridership AS (
FROM read_csv(getvariable('line_data')));
SELECT #1,#2 FROM (DESCRIBE monthly_ridership);
#+END_SRC

#+RESULTS:
#+begin_example
┌─────────────────────────────────┬──────────────────────────┐
│           column_name           │       column_type        │
├─────────────────────────────────┼──────────────────────────┤
│ service_date                    │ TIMESTAMP WITH TIME ZONE │
│ mode                            │ VARCHAR                  │
│ route_or_line                   │ VARCHAR                  │
│ total_monthly_weekday_ridership │ BIGINT                   │
│ average_monthly_weekday_ridersh │ BIGINT                   │
│ countofdates_weekday            │ BIGINT                   │
│ total_monthly_ridership         │ DOUBLE                   │
│ average_monthly_ridership       │ BIGINT                   │
│ countofdates                    │ BIGINT                   │
│ ObjectId                        │ BIGINT                   │
└─────────────────────────────────┴──────────────────────────┘
#+end_example

** List the unique route or lines
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
  FROM monthly_ridership
  SELECT DISTINCT route_or_line
  ORDER BY route_or_line
#+END_SRC

#+RESULTS:
#+begin_example
┌───────────────┐
│ route_or_line │
│    varchar    │
├───────────────┤
│ Blue Line     │
│ Boat-F1       │
│ Boat-F3       │
│ Boat-F4       │
│ Bus           │
│ Commuter Rail │
│ Green Line    │
│ Orange Line   │
│ Red Line      │
│ Silver Line   │
│ The RIDE      │
├───────────────┤
│    11 rows    │
└───────────────┘
#+end_example

** What is the ridership by route or line?
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
We will thus group by route or line to see which routes move around the 
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
SELECT route_or_line, CAST(MEAN(total_monthly_weekday_ridership) AS INTEGER)
AS mean_weekly_ridership FROM monthly_ridership GROUP BY route_or_line
ORDER BY mean_weekly_ridership DESC;
#+END_SRC

#+RESULTS:
#+begin_example
┌───────────────┬───────────────────────┐
│ route_or_line │ mean_weekly_ridership │
│    varchar    │         int32         │
├───────────────┼───────────────────────┤
│ Bus           │               7395469 │
│ Red Line      │               5326706 │
│ Orange Line   │               4410355 │
│ Green Line    │               3893135 │
│ Commuter Rail │               2684943 │
│ Blue Line     │               1392658 │
│ Silver Line   │                724622 │
│ The RIDE      │                138221 │
│ Boat-F1       │                 66248 │
│ Boat-F3       │                 23922 │
│ Boat-F4       │                 19975 │
├───────────────┴───────────────────────┤
│ 11 rows                     2 columns │
└───────────────────────────────────────┘
#+end_example

** Let us look at a table with per-station entries
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
This other dataset contains per-station information
#+begin_src sql :session duckdb :db test.duckdb :exports code :eval no-export :results none
SET VARIABLE gates_data = 'https://hub.arcgis.com
/api/v3/datasets /001c177f07594e7c99f193dde32284c9_0
/downloads/data?format=csv&spatialRefId=4326&where=1%3D1';
#+end_src
** Information of specific entrances
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:

#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
CREATE OR REPLACE TABLE gated_entries AS (
FROM read_csv(getvariable('gates_data')));
SELECT #1,#2 FROM (DESCRIBE gated_entries);
#+END_SRC

#+RESULTS:
#+begin_example
┌───────────────┬──────────────────────────┐
│  column_name  │       column_type        │
│    varchar    │         varchar          │
├───────────────┼──────────────────────────┤
│ service_date  │ TIMESTAMP WITH TIME ZONE │
│ time_period   │ VARCHAR                  │
│ stop_id       │ VARCHAR                  │
│ station_name  │ VARCHAR                  │
│ route_or_line │ VARCHAR                  │
│ gated_entries │ DOUBLE                   │
│ ObjectId      │ BIGINT                   │
└───────────────┴──────────────────────────┘
#+end_example

** What is the station with the most recorded gate entries?
:PROPERTIES:
:BEAMER_opt: shrink=12
:END:
\pause
#+begin_src sql :session duckdb :db test.duckdb :exports both :eval no-export
SELECT station_name,route_or_line,CAST(SUM(gated_entries) AS INT)
AS gated_entries FROM gated_entries
GROUP BY station_name, route_or_line
ORDER BY gated_entries DESC
#+END_SRC

#+RESULTS:
#+begin_example
┌─────────────────┬───────────────┬───────────────┐
│  station_name   │ route_or_line │ gated_entries │
│     varchar     │    varchar    │     int32     │
├─────────────────┼───────────────┼───────────────┤
│ Harvard         │ Red Line      │       4802562 │
│ Back Bay        │ Orange Line   │       4253244 │
│ Copley          │ Green Line    │       3768772 │
│ North Station   │ Orange Line   │       3686858 │
│ Central         │ Red Line      │       3640045 │
│    ·            │    ·          │           ·   │
│    ·            │    ·          │           ·   │
│    ·            │    ·          │           ·   │
│ Suffolk Downs   │ Blue Line     │        234871 │
│ Union Square    │ Green Line    │        181843 │
│ Ball Square     │ Green Line    │        169984 │
│ Magoun Square   │ Green Line    │        148869 │
│ East Somerville │ Green Line    │         89718 │
├─────────────────┴───────────────┴───────────────┤
│ 78 rows (10 shown)                    3 columns │
└─────────────────────────────────────────────────┘
#+end_example

** The one-two: Group and calculate
All data processing is:
1. Group relevant groups
- Treatment vs controls
- Treatments vs each other
2. Math/Logical operation on groups
- Addition
- Cosine similarity
- if -> else flow
* Tools for large-scale data
** What do we consider large-scale?
- Data larger than an off-the-shelf server
- Processing takes time requires memory/storage considerations
- Traditionally we would use cloud compute
** Problem: Many different groups and subgroups
** Abstraction: Tables vs Trees
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:

#+begin_src sql :session duckdb :db test.duckdb :exports both :results raw :wrap :eval no-export 
  SET VARIABLE plates = 'https://github.com/jump-cellpainting/datasets/
  raw/refs/heads/duckdb/metadata/plate.csv.gz';
  FROM getvariable(plates) LIMIT 3;
#+end_src

#+RESULTS:
#+begin_example
┌─────────────────┬─────────────────┬────────────────┬────────────────────┐
│ Metadata_Source │ Metadata_Batch  │ Metadata_Plate │ Metadata_PlateType │
│     varchar     │     varchar     │    varchar     │      varchar       │
├─────────────────┼─────────────────┼────────────────┼────────────────────┤
│ source_1        │ Batch1_20221004 │ UL000109       │ COMPOUND_EMPTY     │
│ source_1        │ Batch1_20221004 │ UL001641       │ COMPOUND           │
│ source_1        │ Batch1_20221004 │ UL001643       │ COMPOUND           │
└─────────────────┴─────────────────┴────────────────┴────────────────────┘
#+end_example

** We can express it as a nested tree
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:
#+begin_src sql :session duckdb :eval no-export :exports code
  COPY (SELECT json_object(
      Metadata_Source, json_group_array(sub_data)
  ) AS root
  FROM (
      SELECT Metadata_Source, Metadata_Batch,
      json_object(Metadata_Batch,
      json_group_array(json_object(Metadata_Plate, Metadata_PlateType))
      ) AS sub_data
      FROM getvariables(plates)
      GROUP BY Metadata_Source, Metadata_Batch, Metadata_Plate
  )
  GROUP BY Metadata_Source) TO 'output.json';
#+end_src

** The equivalent nested json file
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
#+begin_src sh :results verbatim :wrap example :eval no-export :exports both
  jq '.root | walk(if type == "array" then add else . end) |
  with_entries(select(.key == "source_1")) | select(. != {})'
  output.json | head -n 11
#+end_src

#+RESULTS:
#+begin_example
{
  "source_1": {
    "Batch1_20221004": {
      "UL000109": "COMPOUND_EMPTY"
    },
    "Batch2_20221006": {
      "UL001675": "COMPOUND"
    },
    "Batch6_20221102": {
      "UL000599": "COMPOUND"
    },
#+end_example


** Using multiple tables and schemas helps 
 But it slows down when loading >50GB databases onto memory.
** Problem: Massive hierarchical datasets
** Solution: Parquet hive partitioning
** Hive partitioning structure
:PROPERTIES:
:BEAMER_opt: shrink=10
:END:
Single-object (e.g., cell, nuclei) time series data

#+begin_src bash :session test :results verbatim :wrap example :exports both :eval no-export
  DATA_DIR="/datastore/alan/gsk/aliby_output/ELN201687/H00DJKJread1BF48hrs_20230926_095825hive_profiles_tracked"

   tree $DATA_DIR | sed -n "2,10p" 
#+end_src

#+RESULTS:
#+begin_example
├── site=A01_001
│   ├── object=cell
│   │   └── data_0.parquet
│   └── object=nuclei
│       └── data_0.parquet
├── site=A01_002
│   ├── object=cell
│   │   └── data_0.parquet
│   └── object=nuclei
#+end_example

** Hive partitioning
:PROPERTIES:
:BEAMER_opt: shrink=30
:END:
#+begin_src sql :eval no-export :session duckdb :wrap example :results verbatim :exports both 
  SET VARIABLE hive = '/datastore/alan/gsk/aliby_output/ELN201687/
  H00DJKJread1BF48hrs_20230926_095825hive_profiles_tracked/*/*/*.parquet';
  
  FROM read_parquet(getvariable('hive'), hive_partitioning = true) LIMIT 3;
#+end_src

#+RESULTS:
#+begin_example
┌───────┬───────┬───────────────┬───┬─────────┬─────────┬─────────┐
│ tile  │ label │    branch     │ … │ site_1  │ object  │  site   │
│ int64 │ int64 │    varchar    │   │ varchar │ varchar │ varchar │
├───────┼───────┼───────────────┼───┼─────────┼─────────┼─────────┤
│     0 │    33 │ 1/max/texture │ … │ fname   │ cell    │ A01_001 │
│     0 │    33 │ 1/max/texture │ … │ fname   │ cell    │ A01_001 │
│     0 │    33 │ 1/max/texture │ … │ fname   │ cell    │ A01_001 │
├───────┴───────┴───────────────┴───┴─────────┴─────────┴─────────┤
│ 3 rows                                     11 columns (6 shown) │
└─────────────────────────────────────────────────────────────────┘
#+end_example

** Hive partitioning
#+begin_src sql :eval no-export :session duckdb :wrap example :results verbatim  
  SELECT #1,#2 FROM (DESCRIBE FROM read_parquet(getvariable('hive'), hive_partitioning = true));
#+end_src

#+RESULTS:
#+begin_example
┌─────────────┬─────────────┐
│ column_name │ column_type │
│   varchar   │   varchar   │
├─────────────┼─────────────┤
│ tile        │ BIGINT      │
│ label       │ BIGINT      │
│ branch      │ VARCHAR     │
│ metric      │ VARCHAR     │
│ value       │ DOUBLE      │
│ tp          │ UTINYINT    │
│ filename    │ VARCHAR     │
│ fname       │ VARCHAR     │
│ site_1      │ VARCHAR     │
│ object      │ VARCHAR     │
│ site        │ VARCHAR     │
├─────────────┴─────────────┤
│ 11 rows         2 columns │
└───────────────────────────┘
#+end_example

** Disadvantages of Hive partitioning
*** Advantages
- Faster and concurrent write
  Partitions can be written without
- Lower memory footprint
  Only relevant partitions are read into memory
- Faster queries
  Optimisation occurs at the IO level
*** Disadvantages
- Many small files may impact the performance of dataset-wide queries.
- Compression penaty: Compression occurs at the most granular level.
  
** Problem: But my vectors! :(
SQL was invented before vector operations became the pillar of civilization.
** Solution: Extensions 
** Problem: A function not in duckdb
** Solution: User-Defined functions

** Some use-cases
- Dealing with tens/hundreds of GB of data processing for GSK project
- =copairs= 10-100x speedup
- On-browser cosine similarity between Chemical embeddings
- /Claudetanu/ uses it extensively
  
* Conclusions
** Duckdb solves multiple problems
- In-memory OR in disk
- Larger-than memory data
- Multithreading/Multiprocessing
- Optimised queries
- Web requests
- Vector operations
** Rules of the thumb for DuckDB vs GPU
Prefer matrix operations (ideally over GPUs) when
- You need pairwise mathematical operations
- You are already in Python
  
** The ecosystem is still growing
- DuckLake (format): Single repository of data stored in its natural format.
- MotherDuck (startup): Duckdb-based data warehouse in SQL or natural language
  
** I'm excited (or not)! How can I try it out?
From within Python: PyPI's =duckdb= package.
*** In my computers/servers:
- Already on CS-Lab Nix servers
- MacOS: =brew= or Download from =duckdb.org=
- Windows: =duckdb.org=
*** I want a UI!
- The CLI comes with one: =duckdb --ui=
- Marimo supports duckdb out of the box!
- Jupyter notebooks running duckdb
  
** Resources/Blogs
- [[https://www.robinlinacre.com/recommend_duckdb/][Why Duckdb is my first choice for data processing]]
- [[https://www.dbreunig.com/2025/05/03/duckdb-is-the-most-impactful-geospatial-software-in-a-decade.html][DuckDB is Probably the Most Important Geospatial Software of the Last Decade]]
- [[https://hakibenita.com/sql-for-data-analysis][Practical SQL for Data Analysis]]
- [[https://duckdb.org/2024/05/03/vector-similarity-search-vss][Vector Similarity Search in DuckDB]]
- [[https://spectrum.ieee.org/the-rise-of-sql][The Rise of SQL]]
- [[https://www.scattered-thoughts.net/writing/against-sql/][Against SQL]]
